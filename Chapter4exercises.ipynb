{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Stochastic or minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gradient descent training time suffers from differently scaled features.\n",
    "# Regulation is also affected by different scales (features with small values need large weights -> these features are ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. No, logistic regression cost function is convex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. No, very similar, but not the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Too high learning rate or overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. No, it might go down again; only stop it after a long period of non-improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Stochastic is the fastest. Batch will converge. Slowly decrease the learning rate of others to make them converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. It is underfitting. Increase learning rate, feed more data, decrease polynomial degree,\n",
    "# introduce regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. High bias, decrease the regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. a. Regularization good, nonlinear relationship\n",
    "# b. Lasso results in a sparse model / does a feature selection. Use ridge to avoid this effect\n",
    "# c. Elastic net is more stable when there are strongly correlated features or more dimensions than samples, but it adds another thing to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 2 logistic model wince the classes are not exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. GD with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x = iris['data'][:, (2,3)]\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias terms\n",
    "import numpy as np\n",
    "x_bias = np.c_[np.ones([len(x), 1]), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn train_test_split() but without sklearn\n",
    "np.random.seed(2042)\n",
    "\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(x_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "indices = np.random.permutation(total_size)\n",
    "\n",
    "x_train = x_bias[indices[:train_size]]\n",
    "y_train = y[indices[:train_size]]\n",
    "x_valid = x_bias[indices[train_size:-test_size]]\n",
    "y_valid = y[indices[train_size:-test_size]]\n",
    "x_test = x_bias[indices[-test_size:]]\n",
    "y_test = y[indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 2, 1, 1,\n",
       "       1, 2, 0, 0, 0, 2, 0, 1, 2, 0, 2, 1, 1, 2, 1, 0, 0, 1, 2, 2, 2, 2,\n",
       "       0, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 2, 0, 0, 1, 0, 2, 0, 1, 1, 2, 2,\n",
       "       2, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 1, 2, 2, 1, 0, 2, 2, 0, 2, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot 0, 1, 2 target categories\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    y_one_hot = np.zeros((m, n_classes)) # create empty matrix\n",
    "    y_one_hot[np.arange(m), y] = 1 # fill with values\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1h = to_one_hot(y_train)\n",
    "y_valid_1h = to_one_hot(y_valid)\n",
    "y_test_1h = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = x_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.897275838876366\n",
      "1000 0.5425654873413586\n",
      "2000 0.5331256731252507\n",
      "2736 0.5325454243382794\n",
      "2737 0.532545425210158 early stopping\n"
     ]
    }
   ],
   "source": [
    "# Training loop with l2 regularization and early stopping\n",
    "eta = 0.1\n",
    "n_iterations = 5001\n",
    "m = len(x_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularization param\n",
    "best_loss = np.infty\n",
    "\n",
    "theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    logits = x_train.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    x_entropy_loss = -np.mean(np.sum(y_train_1h * np.log(y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(theta[1:]))\n",
    "    loss = x_entropy_loss + alpha * l2_loss\n",
    "    error = y_proba - y_train_1h\n",
    "    gradients = 1/m * x_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * theta[1:]]\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "    # Evaluating the loss with the valid set and stop when error starts growing\n",
    "    logits = x_valid.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "    x_entropy_loss = -np.mean(np.sum(y_valid_1h * np.log(y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(theta[1:]))\n",
    "    loss = x_entropy_loss + alpha * l2_loss\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(i, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(i -1, best_loss)\n",
    "        print(i, loss, 'early stopping')\n",
    "        break   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.66681932, -0.23415483, -5.09025068],\n",
       "       [-1.06640707,  0.14854291,  0.91786416],\n",
       "       [-0.4298001 , -0.13173307,  0.56153317]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "logits = x_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc_score = np.mean(y_predict == y_valid)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
