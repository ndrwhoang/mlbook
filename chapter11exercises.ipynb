{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. No, they would return the same gradients, makes learning impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. No 0/dead neurons, average output at 0 negates the vanishing gradients problem, self normalizing effect for networks with all dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# SELU: networks with all dense layers\n",
    "# leaky ReLU(s): everything else\n",
    "# ReLU: simplicity\n",
    "# tanh: for output layers with result -1 to 1\n",
    "# logistic: for output layers with probability-like output\n",
    "# softmax: for output layers with mutually exclusive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. High momentum can cause it to roll pass optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. l1 regularization, manual forcing weights to 0, and tensorflow optimization all can produce sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Yes, it equals to training many networks. No\n",
    "# Yes, it equals to predicting with many networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Deep neural net with CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:10000], X_train_full[10000:]\n",
    "y_valid, y_train = y_train_full[:10000], y_train_full[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling for non SELU activations\n",
    "X_valid = X_valid.astype('float32') / 255\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for i in range(0, 20):\n",
    "    if i > 15 and i <19:\n",
    "        model.add(Dropout(rate=0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='lecun_normal'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,098\n",
      "Trainable params: 510,154\n",
      "Non-trainable params: 9,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_cb = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cycle scheduling learning rate\n",
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                last_iter=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate/10\n",
    "        self.last_iterations = last_iter or iterations//10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate/1000\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return((rate2-rate1) * (self.iteration-iter1) / (iter2-iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, \n",
    "                                     self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2*self.half_iteration,\n",
    "                                    self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2*self.half_iteration, self.iterations,\n",
    "                                    self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleScheduler(len(X_train) // batch_size*n_epochs, max_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 26s 639us/sample - loss: 1.9900 - accuracy: 0.2776 - val_loss: 1.8556 - val_accuracy: 0.3345\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 19s 484us/sample - loss: 1.7759 - accuracy: 0.3659 - val_loss: 1.7102 - val_accuracy: 0.3957\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 20s 496us/sample - loss: 1.6950 - accuracy: 0.3971 - val_loss: 1.6355 - val_accuracy: 0.4282\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 25s 631us/sample - loss: 1.6412 - accuracy: 0.4185 - val_loss: 1.5677 - val_accuracy: 0.4486\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 26s 662us/sample - loss: 1.5896 - accuracy: 0.4403 - val_loss: 1.5209 - val_accuracy: 0.4582\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 27s 666us/sample - loss: 1.5543 - accuracy: 0.4562 - val_loss: 1.4750 - val_accuracy: 0.4759\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 26s 648us/sample - loss: 1.5227 - accuracy: 0.4694 - val_loss: 1.4634 - val_accuracy: 0.4807\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 26s 662us/sample - loss: 1.4900 - accuracy: 0.4778 - val_loss: 1.4337 - val_accuracy: 0.4950\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 26s 654us/sample - loss: 1.4641 - accuracy: 0.4900 - val_loss: 1.4408 - val_accuracy: 0.5005\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 27s 668us/sample - loss: 1.4457 - accuracy: 0.4973 - val_loss: 1.4512 - val_accuracy: 0.4922\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 27s 668us/sample - loss: 1.4212 - accuracy: 0.5043 - val_loss: 1.4416 - val_accuracy: 0.4895\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 23s 583us/sample - loss: 1.4035 - accuracy: 0.5116 - val_loss: 1.3960 - val_accuracy: 0.5113\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 22s 541us/sample - loss: 1.3855 - accuracy: 0.5164 - val_loss: 1.4166 - val_accuracy: 0.5077\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 21s 532us/sample - loss: 1.3679 - accuracy: 0.5243 - val_loss: 1.4066 - val_accuracy: 0.5041\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 20s 499us/sample - loss: 1.3526 - accuracy: 0.5324 - val_loss: 1.3915 - val_accuracy: 0.5148\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 22s 546us/sample - loss: 1.3404 - accuracy: 0.5362 - val_loss: 1.4111 - val_accuracy: 0.5116\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 23s 577us/sample - loss: 1.3244 - accuracy: 0.5420 - val_loss: 1.4008 - val_accuracy: 0.5117\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 26s 647us/sample - loss: 1.3086 - accuracy: 0.5490 - val_loss: 1.4022 - val_accuracy: 0.5101\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 26s 662us/sample - loss: 1.2897 - accuracy: 0.5555 - val_loss: 1.3804 - val_accuracy: 0.5171\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 27s 663us/sample - loss: 1.2849 - accuracy: 0.5582 - val_loss: 1.4164 - val_accuracy: 0.5071\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 26s 662us/sample - loss: 1.2687 - accuracy: 0.5621 - val_loss: 1.3927 - val_accuracy: 0.5169\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 24s 600us/sample - loss: 1.2514 - accuracy: 0.5678 - val_loss: 1.3928 - val_accuracy: 0.5221\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 21s 530us/sample - loss: 1.2392 - accuracy: 0.5743 - val_loss: 1.3930 - val_accuracy: 0.5241\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 24s 595us/sample - loss: 1.2316 - accuracy: 0.5756 - val_loss: 1.3829 - val_accuracy: 0.5263\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 27s 668us/sample - loss: 1.2268 - accuracy: 0.5773 - val_loss: 1.3943 - val_accuracy: 0.5223\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 27s 663us/sample - loss: 1.2091 - accuracy: 0.5827 - val_loss: 1.3864 - val_accuracy: 0.5240\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 25s 632us/sample - loss: 1.2006 - accuracy: 0.5868 - val_loss: 1.3900 - val_accuracy: 0.5316\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 26s 655us/sample - loss: 1.1878 - accuracy: 0.5923 - val_loss: 1.3748 - val_accuracy: 0.5251\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 25s 633us/sample - loss: 1.1795 - accuracy: 0.5929 - val_loss: 1.3829 - val_accuracy: 0.5291\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 22s 553us/sample - loss: 1.1717 - accuracy: 0.5941 - val_loss: 1.3870 - val_accuracy: 0.5304\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 21s 525us/sample - loss: 1.1607 - accuracy: 0.6023 - val_loss: 1.3828 - val_accuracy: 0.5365\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 20s 491us/sample - loss: 1.1530 - accuracy: 0.6046 - val_loss: 1.3975 - val_accuracy: 0.5285\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 20s 507us/sample - loss: 1.1466 - accuracy: 0.6082 - val_loss: 1.4021 - val_accuracy: 0.5257\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 22s 541us/sample - loss: 1.1328 - accuracy: 0.6141 - val_loss: 1.3763 - val_accuracy: 0.5362\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 24s 601us/sample - loss: 1.1292 - accuracy: 0.6145 - val_loss: 1.4115 - val_accuracy: 0.5184\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 27s 665us/sample - loss: 1.1182 - accuracy: 0.6182 - val_loss: 1.3828 - val_accuracy: 0.5267\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 25s 629us/sample - loss: 1.1172 - accuracy: 0.6156 - val_loss: 1.3937 - val_accuracy: 0.5307\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 22s 543us/sample - loss: 1.1051 - accuracy: 0.6228 - val_loss: 1.4068 - val_accuracy: 0.5266\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=n_epochs,\n",
    "                   batch_size=batch_size,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[early_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 9, 0, ..., 5, 4, 7], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MC Dropout prediction\n",
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
