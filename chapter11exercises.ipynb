{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. No, they would return the same gradients, makes learning impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. No 0/dead neurons, average output at 0 negates the vanishing gradients problem, self normalizing effect for networks with all dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# SELU: networks with all dense layers\n",
    "# leaky ReLU(s): everything else\n",
    "# ReLU: simplicity\n",
    "# tanh: for output layers with result -1 to 1\n",
    "# logistic: for output layers with probability-like output\n",
    "# softmax: for output layers with mutually exclusive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. High momentum can cause it to roll pass optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. l1 regularization, manual forcing weights to 0, and tensorflow optimization all can produce sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Yes, it equals to training many networks. No\n",
    "# Yes, it equals to predicting with many networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Deep neural net with CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:10000], X_train_full[10000:]\n",
    "y_valid, y_train = y_train_full[:10000], y_train_full[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling for non SELU activations\n",
    "X_valid = X_valid.astype('float32') / 255\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for i in range(0, 20):\n",
    "    if i > 15 and i <19:\n",
    "        model.add(Dropout(rate=0.25))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='lecun_normal'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_52 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_53 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_54 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,098\n",
      "Trainable params: 510,154\n",
      "Non-trainable params: 9,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_cb = EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cycle scheduling learning rate\n",
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                last_iter=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate/10\n",
    "        self.last_iterations = last_iter or iterations//10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate/1000\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return((rate2-rate1) * (self.iteration-iter1) / (iter2-iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, \n",
    "                                     self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2*self.half_iteration,\n",
    "                                    self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2*self.half_iteration, self.iterations,\n",
    "                                    self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleScheduler(len(X_train) // batch_size*n_epochs, max_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 27s 666us/sample - loss: 2.0014 - accuracy: 0.2748 - val_loss: 1.8791 - val_accuracy: 0.3408\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 21s 516us/sample - loss: 1.8077 - accuracy: 0.3490 - val_loss: 1.8257 - val_accuracy: 0.3530\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 21s 519us/sample - loss: 1.7307 - accuracy: 0.3856 - val_loss: 1.6864 - val_accuracy: 0.3981\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 20s 511us/sample - loss: 1.6839 - accuracy: 0.4022 - val_loss: 1.6293 - val_accuracy: 0.4125acy: 0.40\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 21s 516us/sample - loss: 1.6542 - accuracy: 0.4166 - val_loss: 1.6400 - val_accuracy: 0.4269- ETA: \n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 21s 518us/sample - loss: 1.6227 - accuracy: 0.4292 - val_loss: 1.5791 - val_accuracy: 0.4492\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 21s 514us/sample - loss: 1.5982 - accuracy: 0.4350 - val_loss: 1.5457 - val_accuracy: 0.4574ss: 1.596 - ETA: 1s - loss: 1.5962 - ac - ETA: 1s - loss: 1.5972 - accura - ETA: \n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 22s 549us/sample - loss: 1.5712 - accuracy: 0.4511 - val_loss: 1.5135 - val_accuracy: 0.4710\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 25s 627us/sample - loss: 1.5545 - accuracy: 0.4572 - val_loss: 1.5264 - val_accuracy: 0.4576\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 27s 683us/sample - loss: 1.5425 - accuracy: 0.4635 - val_loss: 1.4901 - val_accuracy: 0.4710\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 27s 682us/sample - loss: 1.5255 - accuracy: 0.4694 - val_loss: 1.6001 - val_accuracy: 0.4364\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 27s 676us/sample - loss: 1.5006 - accuracy: 0.4780 - val_loss: 1.5102 - val_accuracy: 0.4773\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 27s 666us/sample - loss: 1.4919 - accuracy: 0.4818 - val_loss: 1.4832 - val_accuracy: 0.4777\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 26s 662us/sample - loss: 1.4719 - accuracy: 0.4870 - val_loss: 1.4905 - val_accuracy: 0.4808\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 23s 585us/sample - loss: 1.4720 - accuracy: 0.4892 - val_loss: 1.5251 - val_accuracy: 0.4670\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 22s 545us/sample - loss: 1.4574 - accuracy: 0.4953 - val_loss: 1.5415 - val_accuracy: 0.4780\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 21s 530us/sample - loss: 1.4429 - accuracy: 0.4975 - val_loss: 1.4836 - val_accuracy: 0.4903\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 25s 627us/sample - loss: 1.4313 - accuracy: 0.5041 - val_loss: 1.4751 - val_accuracy: 0.4960\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 27s 683us/sample - loss: 1.4241 - accuracy: 0.5079 - val_loss: 1.5341 - val_accuracy: 0.4753\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 26s 659us/sample - loss: 1.4024 - accuracy: 0.5142 - val_loss: 1.6557 - val_accuracy: 0.4465\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 27s 684us/sample - loss: 1.3998 - accuracy: 0.5135 - val_loss: 1.5281 - val_accuracy: 0.4855\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 27s 665us/sample - loss: 1.3873 - accuracy: 0.5232 - val_loss: 1.4815 - val_accuracy: 0.4891\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 24s 602us/sample - loss: 1.3812 - accuracy: 0.5240 - val_loss: 1.4939 - val_accuracy: 0.5006\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 22s 549us/sample - loss: 1.3756 - accuracy: 0.5272 - val_loss: 1.4951 - val_accuracy: 0.5008\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 21s 514us/sample - loss: 1.3568 - accuracy: 0.5302 - val_loss: 1.4666 - val_accuracy: 0.4929ss: - ETA: 2s - loss: - ETA: 1s - - ETA: 0s - loss: 1.357 - ETA: 0s - loss: 1.3571 - ac\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 20s 507us/sample - loss: 1.3588 - accuracy: 0.5309 - val_loss: 1.4563 - val_accuracy: 0.4993\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 21s 536us/sample - loss: 1.3488 - accuracy: 0.5344 - val_loss: 1.4885 - val_accuracy: 0.5022\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 24s 603us/sample - loss: 1.3344 - accuracy: 0.5434 - val_loss: 1.5577 - val_accuracy: 0.4887\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 26s 654us/sample - loss: 1.3299 - accuracy: 0.5424 - val_loss: 1.5311 - val_accuracy: 0.4950\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 26s 657us/sample - loss: 1.3296 - accuracy: 0.5387 - val_loss: 1.4755 - val_accuracy: 0.4906\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 23s 569us/sample - loss: 1.3239 - accuracy: 0.5442 - val_loss: 1.4581 - val_accuracy: 0.4912\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 21s 534us/sample - loss: 1.3105 - accuracy: 0.5499 - val_loss: 2.0207 - val_accuracy: 0.4909\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 21s 517us/sample - loss: 1.3083 - accuracy: 0.5509 - val_loss: 2.3816 - val_accuracy: 0.4613\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 20s 511us/sample - loss: 1.3028 - accuracy: 0.5515 - val_loss: 2.3754 - val_accuracy: 0.4838loss:\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 21s 537us/sample - loss: 1.3047 - accuracy: 0.5559 - val_loss: 11.7215 - val_accuracy: 0.4996\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 23s 579us/sample - loss: 1.2966 - accuracy: 0.5560 - val_loss: 2.2516 - val_accuracy: 0.4947\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 26s 655us/sample - loss: 1.2816 - accuracy: 0.5619 - val_loss: 28.5865 - val_accuracy: 0.5026\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 26s 660us/sample - loss: 1.2835 - accuracy: 0.5639 - val_loss: 479.0274 - val_accuracy: 0.4848\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 24s 604us/sample - loss: 1.2739 - accuracy: 0.5642 - val_loss: 269.4495 - val_accuracy: 0.4984\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 22s 557us/sample - loss: 1.2817 - accuracy: 0.5617 - val_loss: 9281.6412 - val_accuracy: 0.4826\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 21s 520us/sample - loss: 1.2642 - accuracy: 0.5677 - val_loss: 22550189.8967 - val_accuracy: 0.4654\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 21s 514us/sample - loss: 1.2603 - accuracy: 0.5728 - val_loss: 3595318084.5982 - val_accuracy: 0.4888\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 21s 521us/sample - loss: 1.2571 - accuracy: 0.5685 - val_loss: 9293176453.1416 - val_accuracy: 0.4602\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 23s 565us/sample - loss: 1.2608 - accuracy: 0.5707 - val_loss: 1177778287.7194 - val_accuracy: 0.4949\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 25s 626us/sample - loss: 1.2476 - accuracy: 0.5749 - val_loss: 3547380654529.9346 - val_accuracy: 0.4805\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 27s 669us/sample - loss: 1.2410 - accuracy: 0.5809 - val_loss: 32742856834.1567 - val_accuracy: 0.4958\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 25s 617us/sample - loss: 1.2299 - accuracy: 0.5826 - val_loss: 7230519.5413 - val_accuracy: 0.5042\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 22s 559us/sample - loss: 1.2118 - accuracy: 0.5904 - val_loss: 33118209499.4119 - val_accuracy: 0.4771\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 21s 530us/sample - loss: 1.2047 - accuracy: 0.5919 - val_loss: 81983801948.2349 - val_accuracy: 0.4930\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 21s 516us/sample - loss: 1.1866 - accuracy: 0.5974 - val_loss: 797869008857.6368 - val_accuracy: 0.5070\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 21s 518us/sample - loss: 1.1777 - accuracy: 0.5978 - val_loss: 136412311218.9563 - val_accuracy: 0.5183\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 23s 569us/sample - loss: 1.1646 - accuracy: 0.6036 - val_loss: 11234351153292.0898 - val_accuracy: 0.4909\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 25s 622us/sample - loss: 1.1487 - accuracy: 0.6098 - val_loss: 20103315268574.4883 - val_accuracy: 0.5110\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 28s 708us/sample - loss: 1.1456 - accuracy: 0.6099 - val_loss: 59178778641930.0078 - val_accuracy: 0.5035\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 28s 712us/sample - loss: 1.1190 - accuracy: 0.6188 - val_loss: 2603672023236.1514 - val_accuracy: 0.5100\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 28s 709us/sample - loss: 1.1114 - accuracy: 0.6231 - val_loss: 230334857646.1261 - val_accuracy: 0.5054\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 28s 690us/sample - loss: 1.0988 - accuracy: 0.6251 - val_loss: 716883726415.4139 - val_accuracy: 0.5030\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 27s 684us/sample - loss: 1.0933 - accuracy: 0.6296 - val_loss: 9498891143836.8242 - val_accuracy: 0.5141\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 27s 685us/sample - loss: 1.0795 - accuracy: 0.6330 - val_loss: 118771442341.4655 - val_accuracy: 0.4952\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 28s 696us/sample - loss: 1.0731 - accuracy: 0.6362 - val_loss: 308131288649.1650 - val_accuracy: 0.5112\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 25s 629us/sample - loss: 1.0592 - accuracy: 0.6414 - val_loss: 1543138435.0190 - val_accuracy: 0.5220\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 23s 566us/sample - loss: 1.0481 - accuracy: 0.6439 - val_loss: 225157376759.0307 - val_accuracy: 0.5165\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 21s 533us/sample - loss: 1.0310 - accuracy: 0.6492 - val_loss: 583727725.1311 - val_accuracy: 0.5303\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 21s 513us/sample - loss: 1.0241 - accuracy: 0.6515 - val_loss: 5858046471060.8203 - val_accuracy: 0.5088\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 21s 515us/sample - loss: 1.0141 - accuracy: 0.6553 - val_loss: 3947079161728.1489 - val_accuracy: 0.5192\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 23s 563us/sample - loss: 1.0057 - accuracy: 0.6564 - val_loss: 1796497676644.4297 - val_accuracy: 0.5093\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 24s 598us/sample - loss: 0.9973 - accuracy: 0.6612 - val_loss: 54674767677.8324 - val_accuracy: 0.5189\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 28s 710us/sample - loss: 0.9871 - accuracy: 0.6621 - val_loss: 199361627369.0845 - val_accuracy: 0.5153\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 26s 644us/sample - loss: 0.9725 - accuracy: 0.6664 - val_loss: 599703246620.4999 - val_accuracy: 0.5157\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 23s 567us/sample - loss: 0.9727 - accuracy: 0.6692 - val_loss: 214894744561.4309 - val_accuracy: 0.5141\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 22s 538us/sample - loss: 0.9671 - accuracy: 0.6702 - val_loss: 19550307947.5145 - val_accuracy: 0.5127\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 20s 508us/sample - loss: 0.9545 - accuracy: 0.6771 - val_loss: 275639875790.7656 - val_accuracy: 0.5083541 - ac\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 21s 515us/sample - loss: 0.9457 - accuracy: 0.6783 - val_loss: 150835304826.4912 - val_accuracy: 0.5119\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 20s 509us/sample - loss: 0.9273 - accuracy: 0.6852 - val_loss: 37747896.3658 - val_accuracy: 0.5318\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 21s 530us/sample - loss: 0.9311 - accuracy: 0.6810 - val_loss: 149828468345.6800 - val_accuracy: 0.5112\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 24s 597us/sample - loss: 0.9171 - accuracy: 0.6827 - val_loss: 26958460627.3268 - val_accuracy: 0.5215\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 26s 647us/sample - loss: 0.9042 - accuracy: 0.6909 - val_loss: 15277309185.1733 - val_accuracy: 0.5324\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 24s 596us/sample - loss: 0.9031 - accuracy: 0.6921 - val_loss: 60669572911.6283 - val_accuracy: 0.5156\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 22s 540us/sample - loss: 0.8957 - accuracy: 0.6952 - val_loss: 106032977945.2152 - val_accuracy: 0.5166\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 21s 514us/sample - loss: 0.8822 - accuracy: 0.7003 - val_loss: 126466480669.5589 - val_accuracy: 0.5194 0.8832 - accuracy:  - ETA: 0s - loss: 0.8821 - accuracy: 0. - ETA: 0s - loss: 0.8824 - accu\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 20s 512us/sample - loss: 0.8866 - accuracy: 0.6992 - val_loss: 135156650188.9028 - val_accuracy: 0.5181\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 20s 507us/sample - loss: 0.8703 - accuracy: 0.7030 - val_loss: 160517187492.9507 - val_accuracy: 0.5184loss: 0.8648 - ac - ETA: - ETA: 1s - loss: 0.8721 - accu - ETA: 0s - los\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 21s 513us/sample - loss: 0.8576 - accuracy: 0.7059 - val_loss: 60923977219.0744 - val_accuracy: 0.5260\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 23s 566us/sample - loss: 0.8540 - accuracy: 0.7067 - val_loss: 45740575374.9595 - val_accuracy: 0.5228\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 26s 661us/sample - loss: 0.8484 - accuracy: 0.7099 - val_loss: 8208214674.8971 - val_accuracy: 0.5294\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 24s 600us/sample - loss: 0.8393 - accuracy: 0.7138 - val_loss: 127466900781.7162 - val_accuracy: 0.5247\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 22s 542us/sample - loss: 0.8319 - accuracy: 0.7153 - val_loss: 110694198638.5810 - val_accuracy: 0.5240\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 21s 514us/sample - loss: 0.8284 - accuracy: 0.7184 - val_loss: 94730217668.8638 - val_accuracy: 0.5181\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 20s 507us/sample - loss: 0.8139 - accuracy: 0.7207 - val_loss: 60128977903.3757 - val_accuracy: 0.5189\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 20s 508us/sample - loss: 0.8150 - accuracy: 0.7203 - val_loss: 26492326344.9553 - val_accuracy: 0.5232y: 0.71 - ETA:  - ETA: 3s - loss: 0.8127 - accuracy:  - ETA: 0s - loss:\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 20s 507us/sample - loss: 0.8074 - accuracy: 0.7222 - val_loss: 47270069653.3236 - val_accuracy: 0.5260\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 24s 604us/sample - loss: 0.7967 - accuracy: 0.7272 - val_loss: 33257692590.5389 - val_accuracy: 0.5279\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 30s 748us/sample - loss: 0.8041 - accuracy: 0.7240 - val_loss: 16229490339.7936 - val_accuracy: 0.5254\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 28s 709us/sample - loss: 0.7872 - accuracy: 0.7290 - val_loss: 77902199547.6545 - val_accuracy: 0.5268\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 29s 721us/sample - loss: 0.7880 - accuracy: 0.7301 - val_loss: 114249838317.6913 - val_accuracy: 0.5250\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 29s 727us/sample - loss: 0.7826 - accuracy: 0.7308 - val_loss: 61134376949.0974 - val_accuracy: 0.5275\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 29s 725us/sample - loss: 0.7747 - accuracy: 0.7354 - val_loss: 39835450013.9442 - val_accuracy: 0.5289\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 29s 720us/sample - loss: 0.7824 - accuracy: 0.7323 - val_loss: 147779839734.7390 - val_accuracy: 0.5269\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 32s 800us/sample - loss: 0.7732 - accuracy: 0.7346 - val_loss: 148856921867.4441 - val_accuracy: 0.5256\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 29s 714us/sample - loss: 0.7772 - accuracy: 0.7347 - val_loss: 91168270785.2187 - val_accuracy: 0.5259\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                   epochs=n_epochs,\n",
    "                   batch_size=batch_size,\n",
    "                   validation_data=(X_valid, y_valid),\n",
    "                   callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 0, ..., 3, 4, 7], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MC Dropout prediction\n",
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5395"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
