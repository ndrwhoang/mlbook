{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x2b47ef6d358>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n",
    "Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "# or\n",
    "he_avg_init = VarianceScaling(scale=2., mode='fan_avg',   # avg of input and output/no of neurons of each layer\n",
    "                             distribution='uniform')\n",
    "Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementations of activation functions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky ReLU\n",
    "#(for keras implementaiton add a LeakyReLU() layer)\n",
    "def leaky_relu(z, alpha=0.03):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU\n",
    "#(for keras implementation set actvation='elu')\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z<0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "#(might not work well with l1, l2 regularization, dropout, max-norm, non-sequential architecture)\n",
    "#(works well with sequential CNN)\n",
    "#(for keras specify activation='selu' and kernel_initializer='lecun_normal')\n",
    "from scipy.special import erfc\n",
    "\n",
    "alpha_0_1 = -np.sqrt(2/np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1/np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2*np.pi) * (2*erfc(np.sqrt(2))*np.e**2 + \n",
    "            np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e) + \n",
    "            np.pi+2)**(-1/2)\n",
    "\n",
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn to use the optimal scaling and shifting for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each batch in training\n",
    "# 1.compute the mean\n",
    "# 2.compute the std\n",
    "# 3.compute zero centered and normalized values\n",
    "# 4.compute the optimal scale and mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prediction, either\n",
    "# use the mean and std of the whole training set\n",
    "# (in actual implementations, computes a moving mean and std during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + also provides some regularization\n",
    "# - adds complexity\n",
    "# - adds computation time in prediction\n",
    "#(mitigated by updating the weights and biases with also normalized weights ans biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Norm in keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x2b47f2d9f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_3/gamma:0', True),\n",
       " ('batch_normalization_3/beta:0', True),\n",
       " ('batch_normalization_3/moving_mean:0', False),\n",
       " ('batch_normalization_3/moving_variance:0', False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative, put batch norm layers before activation functions\n",
    "#(remove activation functions from the layers and make own seperate layers)\n",
    "#(remove biases)\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Clipping\n",
    "#(clip gradient values outside some threshold)\n",
    "#(frequntly used in RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping in keras\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1)   #alternatively use clipnorm\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset set up\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255\n",
    "X_test = X_test / 255\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "#(Set A: images without sandals and shirts)\n",
    "#(Set B: images of only sandals and shirts)\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y==5) | (y==6)   #(5, 6:labels for sandals and shirts)\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2   #label indexes adjustment\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)\n",
    "    \n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "           (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models set up\n",
    "model_A = Sequential()\n",
    "model_A.add(Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(Dense(n_hidden, activation='selu'))\n",
    "model_A.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model_A.compile(loss='sparse_categorical_crossentropy',\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 6s 143us/sample - loss: 0.5909 - accuracy: 0.8101 - val_loss: 0.3772 - val_accuracy: 0.8707\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 5s 104us/sample - loss: 0.3520 - accuracy: 0.8791 - val_loss: 0.3395 - val_accuracy: 0.8779\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 5s 107us/sample - loss: 0.3167 - accuracy: 0.8891 - val_loss: 0.3017 - val_accuracy: 0.8969\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 5s 103us/sample - loss: 0.2970 - accuracy: 0.8966 - val_loss: 0.2858 - val_accuracy: 0.9031\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 5s 106us/sample - loss: 0.2825 - accuracy: 0.9030 - val_loss: 0.2809 - val_accuracy: 0.9063\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 5s 110us/sample - loss: 0.2721 - accuracy: 0.9073 - val_loss: 0.2683 - val_accuracy: 0.9106\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 5s 108us/sample - loss: 0.2642 - accuracy: 0.9098 - val_loss: 0.2683 - val_accuracy: 0.9108\n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 5s 106us/sample - loss: 0.2570 - accuracy: 0.9118 - val_loss: 0.2737 - val_accuracy: 0.9046\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 5s 105us/sample - loss: 0.2514 - accuracy: 0.9139 - val_loss: 0.2590 - val_accuracy: 0.9116\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 5s 106us/sample - loss: 0.2456 - accuracy: 0.9161 - val_loss: 0.2597 - val_accuracy: 0.9108\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 5s 116us/sample - loss: 0.2416 - accuracy: 0.9180 - val_loss: 0.2489 - val_accuracy: 0.9160\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 6s 142us/sample - loss: 0.2378 - accuracy: 0.9187 - val_loss: 0.2449 - val_accuracy: 0.9160\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 6s 147us/sample - loss: 0.2343 - accuracy: 0.9204 - val_loss: 0.2453 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 7s 150us/sample - loss: 0.2308 - accuracy: 0.9206 - val_loss: 0.2440 - val_accuracy: 0.9163\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 7s 156us/sample - loss: 0.2278 - accuracy: 0.9217 - val_loss: 0.2393 - val_accuracy: 0.9188\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 7s 153us/sample - loss: 0.2249 - accuracy: 0.9225 - val_loss: 0.2392 - val_accuracy: 0.9180\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 7s 150us/sample - loss: 0.2218 - accuracy: 0.9238 - val_loss: 0.2390 - val_accuracy: 0.9178\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 7s 148us/sample - loss: 0.2199 - accuracy: 0.9242 - val_loss: 0.2403 - val_accuracy: 0.9175\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 7s 150us/sample - loss: 0.2177 - accuracy: 0.9254 - val_loss: 0.2331 - val_accuracy: 0.9208\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 7s 153us/sample - loss: 0.2154 - accuracy: 0.9254 - val_loss: 0.2329 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                     validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = Sequential()\n",
    "model_B.add(Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(Dense(n_hidden, activation='selu'))\n",
    "model_B.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_B.compile(loss='binary_crossentropy',\n",
    "               optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.9545 - accuracy: 0.4600 - val_loss: 0.6655 - val_accuracy: 0.5385\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 494us/sample - loss: 0.5899 - accuracy: 0.6900 - val_loss: 0.4785 - val_accuracy: 0.8519\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 534us/sample - loss: 0.4512 - accuracy: 0.8800 - val_loss: 0.4098 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 578us/sample - loss: 0.3871 - accuracy: 0.9100 - val_loss: 0.3666 - val_accuracy: 0.9128\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 524us/sample - loss: 0.3438 - accuracy: 0.9250 - val_loss: 0.3315 - val_accuracy: 0.9300\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 494us/sample - loss: 0.3095 - accuracy: 0.9300 - val_loss: 0.3034 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 517us/sample - loss: 0.2810 - accuracy: 0.9400 - val_loss: 0.2808 - val_accuracy: 0.9432\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 522us/sample - loss: 0.2580 - accuracy: 0.9500 - val_loss: 0.2618 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 0s 559us/sample - loss: 0.2372 - accuracy: 0.9600 - val_loss: 0.2447 - val_accuracy: 0.9513\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 553us/sample - loss: 0.2196 - accuracy: 0.9650 - val_loss: 0.2316 - val_accuracy: 0.9513\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 501us/sample - loss: 0.2048 - accuracy: 0.9650 - val_loss: 0.2182 - val_accuracy: 0.9533\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 511us/sample - loss: 0.1915 - accuracy: 0.9650 - val_loss: 0.2071 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 499us/sample - loss: 0.1791 - accuracy: 0.9650 - val_loss: 0.1959 - val_accuracy: 0.9594\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 518us/sample - loss: 0.1681 - accuracy: 0.9700 - val_loss: 0.1864 - val_accuracy: 0.9604\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 559us/sample - loss: 0.1579 - accuracy: 0.9850 - val_loss: 0.1778 - val_accuracy: 0.9625\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 499us/sample - loss: 0.1491 - accuracy: 0.9850 - val_loss: 0.1695 - val_accuracy: 0.9675\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 524us/sample - loss: 0.1411 - accuracy: 0.9900 - val_loss: 0.1626 - val_accuracy: 0.9686\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 583us/sample - loss: 0.1339 - accuracy: 0.9900 - val_loss: 0.1570 - val_accuracy: 0.9686\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 581us/sample - loss: 0.1277 - accuracy: 0.9900 - val_loss: 0.1522 - val_accuracy: 0.9696\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 504us/sample - loss: 0.1217 - accuracy: 0.9900 - val_loss: 0.1471 - val_accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20, \n",
    "                     validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use model A for task B\n",
    "# Remove output layer\n",
    "model_B_on_A = Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(Dense(1, activation='sigmoid'))\n",
    "# Save a clone of model A\n",
    "#(since training B_on_A will affect A)\n",
    "model_A_clone = keras.models.clone_model(model_A)   #clone architecture\n",
    "model_A_clone.set_weights(model_A.get_weights())   #clone weights\n",
    "# Freezing layers\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 1s 3ms/sample - loss: 0.1836 - accuracy: 0.9600 - val_loss: 0.1753 - val_accuracy: 0.9807\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 504us/sample - loss: 0.1447 - accuracy: 0.9700 - val_loss: 0.1476 - val_accuracy: 0.9807\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 524us/sample - loss: 0.1206 - accuracy: 0.9850 - val_loss: 0.1282 - val_accuracy: 0.9838\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 485us/sample - loss: 0.1036 - accuracy: 0.9850 - val_loss: 0.1143 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "# Train for a few epochs\n",
    "#(to let the last layer learn some weights)\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                          validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze and train\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4)   #reduced learning rate to avoid the transfered weights being changed drastically\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.0950 - accuracy: 0.9850 - val_loss: 0.1134 - val_accuracy: 0.9858\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 0s 522us/sample - loss: 0.0940 - accuracy: 0.9850 - val_loss: 0.1126 - val_accuracy: 0.9858\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 0s 491us/sample - loss: 0.0932 - accuracy: 0.9850 - val_loss: 0.1117 - val_accuracy: 0.9858\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 0s 531us/sample - loss: 0.0923 - accuracy: 0.9850 - val_loss: 0.1109 - val_accuracy: 0.9858\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 0s 509us/sample - loss: 0.0915 - accuracy: 0.9850 - val_loss: 0.1101 - val_accuracy: 0.9858\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 0s 469us/sample - loss: 0.0906 - accuracy: 0.9850 - val_loss: 0.1093 - val_accuracy: 0.9858\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 0s 479us/sample - loss: 0.0897 - accuracy: 0.9850 - val_loss: 0.1084 - val_accuracy: 0.9858\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 0s 529us/sample - loss: 0.0888 - accuracy: 0.9850 - val_loss: 0.1076 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 0s 539us/sample - loss: 0.0879 - accuracy: 0.9850 - val_loss: 0.1069 - val_accuracy: 0.9858\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 0s 472us/sample - loss: 0.0872 - accuracy: 0.9850 - val_loss: 0.1061 - val_accuracy: 0.9858\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 0s 524us/sample - loss: 0.0864 - accuracy: 0.9850 - val_loss: 0.1054 - val_accuracy: 0.9858\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 0s 499us/sample - loss: 0.0857 - accuracy: 0.9850 - val_loss: 0.1047 - val_accuracy: 0.9858\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 0s 552us/sample - loss: 0.0849 - accuracy: 0.9850 - val_loss: 0.1039 - val_accuracy: 0.9858\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 0s 568us/sample - loss: 0.0841 - accuracy: 0.9850 - val_loss: 0.1032 - val_accuracy: 0.9858\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 0s 524us/sample - loss: 0.0834 - accuracy: 0.9850 - val_loss: 0.1026 - val_accuracy: 0.9858\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 0s 506us/sample - loss: 0.0827 - accuracy: 0.9850 - val_loss: 0.1019 - val_accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                          validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum Optimization\n",
    "#1.uses the gradient to determine a momentum vector\n",
    "#2.updates weights with the momentum vector\n",
    "#(has a 'friction' hyperparam, is like a learning rate)\n",
    "#(almost always faster than gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum optimization in keras\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nesterov Accelerated Gradient\n",
    "#(momentum optimization but calculate the gradient of the local gradient plus the momentum)\n",
    "#(this works because the original gradient direction is outdated when computing the momentum vector)\n",
    "#((because the weight updates are already moved in the direction of the momentum before the local gradient is taken into account))\n",
    "#(almost always faster than momentum optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAG in keras\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "#(scales the weight updates to the steepness of the dimensions)\n",
    "#(equivalently decays the learning rate for dimensions with higher steepness)\n",
    "#1.calculates a scaler being a matrix of squared gradients of each param/dimension\n",
    "#(through iterations the scaler also gets updated with new gradients)\n",
    "#2.updates the gradients with the gradient vector scaled (divided by the sqrt sum of the scaler and a smoothing term)\n",
    "# - might stop training before convergence because of increasing decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "#(adagrad but with a decay element to decay the earlier gradients)\n",
    "#(only uses gradients from recent iterations to update the scaler)\n",
    "#(almost always better than AdaGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp in keras\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam and Nadam\n",
    "#(momentum and rmsprop combined, like momentum but scaled)\n",
    "# 1.Calculate a momentum vector (with decaying average rather than sum)\n",
    "# 2.Calculate a scaling thing (exponentially decaying scaling thing)\n",
    "# 3.Filler step to speed up training for momentum\n",
    "# 4.Filler step to speed up training for scaling\n",
    "# 5.Weight update with scaled momentum\n",
    "#(learning rate doesnt need much tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam in keras\n",
    "optimizer = keras.optimizers.Adam(lr=0.001,\n",
    "                                 beta_1 = 0.9,   #decaying rate for momentum\n",
    "                                 beta_2=0.999)   #decaying rate for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam alternatives\n",
    "# AdaMax\n",
    "#(use l-infinity (the max) rather than l2 norm to scale)\n",
    "#(might be more stable than Adam, might not)\n",
    "# Nadam\n",
    "#(Like Adam but with Nesterov rather than vanilla momentum)\n",
    "#(Generally better than Adam, but can be worse than RMSProp?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sidenotes\n",
    "#(adaptive method might generalize poorly, in which case use nesterov)\n",
    "#(there are optimizers that use second order partial derivatives, but they are slow)\n",
    "#(apply strong l1 regularization during training to get sparse model (which is faster))\n",
    "#(avoid SGD and adagrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power scheduling\n",
    "#(learning rate drops at each step)\n",
    "#(first drops quickly and then more slowly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power scheduling in keras\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential scheduling\n",
    "#(learning rate drops by a factor of 10 every s steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential scheduling not in keras\n",
    "#(passes in initial learning rate and steps before decay,\n",
    "#returns a learning rate for an epoch)\n",
    "def expo_decay(lr0, s):\n",
    "    def expo_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exp_decay_fn\n",
    "\n",
    "expo_decay_fn = expo_decay(lr0=0.01, s=20)\n",
    "\n",
    "# Use the function in a callback\n",
    "#(learning rate is updated at the beginning at every epoch)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(expo_decay_fn)\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential scheduling in tf.keras\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, 20*len(X_train), 0.1)\n",
    "optimizer = keras.optimizer.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise constant scheduling\n",
    "#(use a fixed learning ratefor a fixed number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pcs, also not in keras\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "# Alternatively\n",
    "#(passes in an array length n of epoch index, array n+1 of learning rate)\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0]+boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch)-1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "# Make callback\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scheduling\n",
    "#(measure error every n steps, reduce learning rate by a factor if not dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scheduling in keras\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cycle scheduling\n",
    "#(increases the learning rate linearly to a maximum n for half of training)\n",
    "#(decreases for the second half, decreases drastically for last few epochs)\n",
    "#(inital learning rate is typically about 10 times lower than max)\n",
    "#(when using momentum, do the reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1cs also also not in keras\n",
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2-rate1) * (self.iteration-iter1) / (iter2-iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 an l2 regularization\n",
    "# l2 in keras\n",
    "layers = Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "              kernel_regularizer=keras.regularizers.l2(0.01))   # for l1 use regularizers.l1(), l1_l2() for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial() to create architecture with the same layer type, activation, etc.\n",
    "from functools import partial\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                          activation='elu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "#(dropout rate 0.2-0.3 for RNN, 0.4-0.5 for CNN)\n",
    "#(in practice usually apply dropout to top 1 to 3 layers excluding output(?))\n",
    "#(multiply input weight or divide neuron output, by keep proba in prediction time)\n",
    "#(re-evaluate training loss if needed after training (when dropout is not applied anymore))\n",
    "# - slows down convergence, but worth it\n",
    "#(use alpha dropout for selu activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout in keras\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    Dropout(rate=0.2),\n",
    "    Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dropout(rate=0.2),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dropout(rate=0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Dropout\n",
    "#(ensemble of dropout neural nets)\n",
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])   #generate 100 predictions\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_probas[:, :1], 2)   #a prediction probas\n",
    "np.round(y_proba[:1], 2)   #mean proba of the same prediction\n",
    "\n",
    "y_std = y_probas.std(axis=0)   #std of predictions\n",
    "np.round(y_std[:1], 2)   #std of the above prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCDropout class\n",
    "#(for when there are input manipulation layers other than Dropout, like batchnorm)\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "class MCDropout(Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_dropout_3 (MCDropout)     (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_dropout_4 (MCDropout)     (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_dropout_5 (MCDropout)     (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Copy model over, replace orginal dropout layers\n",
    "mc_model = Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, keras.layers.Dropout) else layer for layer in model.layers\n",
    "])\n",
    "mc_model.set_weights(model.get_weights())\n",
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo dropout with possible batchnorm\n",
    "#(not forcing training=True)\n",
    "y_proba = np.mean([mc_model.predict(X_test) for sample in range(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max-Norm Regularization\n",
    "#(constraint the L2-ed weights under a max-norm r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x15b7affa6a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max-Norm in keras\n",
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "                  kernel_constraint=keras.constraints.max_norm(1, axis=0))\n",
    "#(when max-norm-ing convolutional layers, change hyperparam axis=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended Settings\n",
    "#(simple dense network settings in brackets)\n",
    "# Kernel initializer: he init  (lecun init)\n",
    "# Activation function: elu   (selu)\n",
    "# Regularization: early stopping and l2   (alpha dropout)\n",
    "# Optimizer: momentum, rmsprop, or nadam   (same)\n",
    "# Learning rate schedule: 1cycle   (same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More stuff\n",
    "#(normalize input features)\n",
    "#(find a pretrained model)\n",
    "#(unsupervised pretraining for unlabeled data)\n",
    "#(pretrain on auxiliary on similar task)\n",
    "#(use l1 regularization for sparse model)\n",
    "#(use fewer layers, fold batchnorm into previous layer, use leaky relu or relu, sparse model, \n",
    "#reduce float precision from 32 to 16 or 8 \n",
    "#to speed up predition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
