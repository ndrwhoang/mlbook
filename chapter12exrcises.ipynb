{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TF is used for numerical computation like numpy but with GPU support and graphs and optimizations\n",
    "# Similar libarries: pytorch, theano, caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Numpy and tf have different syntaxes and behave different in some ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Yes with data type format differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Subclass keras loss if the custom loss functions need to work with hyperparameters or state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Subclass metrics if the custom metrics have the same requirements above, and the metric results are different computed across eah epoch vs the whole dataset/need to have running metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. The model is the thing being trained, the layers are the things inside them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. For debugging or using multiple optimizers for wide architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Keras components can contain Python code, which can be converted to a limited extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. No items fox only final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Manual layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNormalization(keras.layers.Layer):\n",
    "    def __init(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma',\n",
    "                                    shape=(input_shape[-1:]),\n",
    "                                    initializer=keras.initializers.Ones(),\n",
    "                                    trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                   shape=(input_shape[-1:]),\n",
    "                                   initializer=keras.initializers.Zeroes(),\n",
    "                                   trainable=True)\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, X):\n",
    "        mean, var = nn.moments(inputs, axes=-1, keepdims=True)\n",
    "        return self.activation(self.alpha@(X-mean) / (tf.math.sqrt(var)+0.001) + self.beta)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units':self.units, \n",
    "               'activation': keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Custom training loop on fashion MNIST\n",
    "#(displays epoch, iteration, mean training loss, mean accuracy for each epoch,\n",
    "# updates at each iteration)\n",
    "#(displays validation loss and accuracy at the end of each epoch)\n",
    "#(use a different optimizer and learning rate for upper and lower layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = Sequential([\n",
    "    Dense(30, activation='elu', kernel_initializer='he_normal',\n",
    "         kernel_regularizer=l2_reg),\n",
    "    Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status(iteration, total, loss, metrics=None):\n",
    "    metrics = ' - '.join(['{}:{:.2f}'.format(metric.name, metric.result())\n",
    "                         for metric in [loss] + (metrics or [])])\n",
    "    end = '' if iteration < total else '\\n'\n",
    "    print('\\r{}\\{} - '.format(iteration, total) + metrics,\n",
    "         end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_iterations = len(X_train)//batch_size\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.Accuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    print('Epoch {}/{}'.format(epoch, n_epochs))\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = random_batch(X_train, y_batch)\n",
    "        with tf.GradientTape as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.loses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status(iteration*batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
